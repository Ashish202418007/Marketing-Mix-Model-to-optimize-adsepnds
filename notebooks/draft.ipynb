{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba493a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer import MCMC, NUTS, Predictive\n",
    "from numpyro.infer.autoguide import AutoLowRankMultivariateNormal\n",
    "from numpyro.infer import SVI, Trace_ELBO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Optional, Dict, Any, Tuple\n",
    "from scipy.optimize import minimize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class EnhancedLightweightMMM:\n",
    "    \"\"\"\n",
    "    Enhanced LightweightMMM with advanced features:\n",
    "    - Competitive effects\n",
    "    - Cross-channel interactions\n",
    "    - Time-varying coefficients\n",
    "    - Power transformations\n",
    "    - Risk-adjusted optimization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model_name: str = \"enhanced_hill_adstock\",\n",
    "                 degrees_seasonality: int = 2,\n",
    "                 weekday_seasonality: bool = True,\n",
    "                 adstock_max_lag: int = 7,\n",
    "                 convolve_func: str = \"Adstock\"):\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.degrees_seasonality = degrees_seasonality\n",
    "        self.weekday_seasonality = weekday_seasonality\n",
    "        self.adstock_max_lag = adstock_max_lag\n",
    "        self.convolve_func = convolve_func\n",
    "        self.mcmc = None\n",
    "        self.posterior_samples = None\n",
    "        self.guide = None\n",
    "        self.svi = None\n",
    "        \n",
    "    def _apply_power_adstock(self, x: jnp.ndarray, lambda_adstock: float, rho_power: float) -> jnp.ndarray:\n",
    "        \"\"\"Apply power-transformed adstock transformation\"\"\"\n",
    "        def scan_fn(carry, x_t):\n",
    "            adstock_t = x_t**rho_power + lambda_adstock * carry\n",
    "            return adstock_t, adstock_t\n",
    "        \n",
    "        _, adstock_data = jax.lax.scan(scan_fn, init=0.0, xs=x)\n",
    "        return adstock_data\n",
    "    \n",
    "    def _apply_hill_saturation(self, x: jnp.ndarray, K_sat: float, S_sat: float, nu_sat: float) -> jnp.ndarray:\n",
    "        \"\"\"Apply Hill saturation transformation\"\"\"\n",
    "        denominator = jnp.power(S_sat, nu_sat) + jnp.power(x, nu_sat)\n",
    "        return K_sat * jnp.power(x, nu_sat) / denominator\n",
    "    \n",
    "    def _apply_competitive_effect(self, x: jnp.ndarray, competitor_spend: jnp.ndarray, \n",
    "                                 phi_comp: float, kappa_comp: float) -> jnp.ndarray:\n",
    "        \"\"\"Apply competitive interference effect\"\"\"\n",
    "        if competitor_spend is None:\n",
    "            return x\n",
    "        \n",
    "        competitive_saturation = jnp.power(competitor_spend, phi_comp) / (\n",
    "            jnp.power(competitor_spend, phi_comp) + jnp.power(kappa_comp, phi_comp)\n",
    "        )\n",
    "        competitive_factor = 1 - competitive_saturation\n",
    "        return x * competitive_factor\n",
    "    \n",
    "    def _create_seasonality_features(self, n_time_periods: int) -> jnp.ndarray:\n",
    "        \"\"\"Create seasonality features\"\"\"\n",
    "        time_arange = jnp.arange(n_time_periods, dtype=jnp.float32)\n",
    "        \n",
    "        # Annual seasonality\n",
    "        seasonality_features = []\n",
    "        for i in range(1, self.degrees_seasonality + 1):\n",
    "            cos_features = jnp.cos(2 * jnp.pi * i * time_arange / 52.0)\n",
    "            sin_features = jnp.sin(2 * jnp.pi * i * time_arange / 52.0)\n",
    "            seasonality_features.extend([cos_features, sin_features])\n",
    "        \n",
    "        # Weekday seasonality\n",
    "        if self.weekday_seasonality:\n",
    "            for i in range(1, 4):  # 3 degrees for weekday\n",
    "                cos_features = jnp.cos(2 * jnp.pi * i * time_arange / 7.0)\n",
    "                sin_features = jnp.sin(2 * jnp.pi * i * time_arange / 7.0)\n",
    "                seasonality_features.extend([cos_features, sin_features])\n",
    "        \n",
    "        return jnp.stack(seasonality_features, axis=-1)\n",
    "    \n",
    "    def _compute_cross_channel_interactions(self, transformed_media: jnp.ndarray, \n",
    "                                          delta_interact: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"Compute pairwise cross-channel interactions\"\"\"\n",
    "        n_media_channels = transformed_media.shape[1]\n",
    "        interaction_contrib = 0.0\n",
    "        \n",
    "        idx = 0\n",
    "        for i in range(n_media_channels):\n",
    "            for j in range(i + 1, n_media_channels):\n",
    "                interaction_contrib += (delta_interact[idx] * \n",
    "                                      transformed_media[:, i] * \n",
    "                                      transformed_media[:, j])\n",
    "                idx += 1\n",
    "        \n",
    "        return interaction_contrib\n",
    "    \n",
    "    def _model_definition(self, media_data: jnp.ndarray, \n",
    "                         extra_features: Optional[jnp.ndarray] = None,\n",
    "                         competitor_data: Optional[jnp.ndarray] = None,\n",
    "                         target_data: Optional[jnp.ndarray] = None) -> None:\n",
    "        \"\"\"Enhanced LightweightMMM model definition\"\"\"\n",
    "        \n",
    "        n_time_periods, n_media_channels = media_data.shape\n",
    "        n_extra_features = extra_features.shape[1] if extra_features is not None else 0\n",
    "        n_interactions = n_media_channels * (n_media_channels - 1) // 2\n",
    "        \n",
    "        # Seasonality features\n",
    "        seasonality_features = self._create_seasonality_features(n_time_periods)\n",
    "        n_seasonality_features = seasonality_features.shape[1]\n",
    "        \n",
    "        # === PRIORS ===\n",
    "        \n",
    "        # Intercept\n",
    "        intercept = numpyro.sample(\"intercept\", dist.Normal(0, 1))\n",
    "        \n",
    "        # Trend\n",
    "        trend_coef = numpyro.sample(\"trend_coef\", dist.Normal(0, 0.1))\n",
    "        \n",
    "        # Seasonality coefficients\n",
    "        seasonality_coef = numpyro.sample(\"seasonality_coef\", \n",
    "                                        dist.Normal(0, 0.1).expand([n_seasonality_features]))\n",
    "        \n",
    "        # Media channel parameters\n",
    "        with numpyro.plate(\"media_channels\", n_media_channels):\n",
    "            # Base media coefficients (positive)\n",
    "            beta_media = numpyro.sample(\"beta_media\", dist.Gamma(1, 1))\n",
    "            \n",
    "            # Time-varying seasonality multiplier\n",
    "            seasonal_multiplier = numpyro.sample(\"seasonal_multiplier\", dist.Normal(0, 0.1))\n",
    "            \n",
    "            # Adstock parameters\n",
    "            lambda_adstock = numpyro.sample(\"lambda_adstock\", dist.Beta(2, 2))\n",
    "            rho_power = numpyro.sample(\"rho_power\", \n",
    "                                     dist.TruncatedNormal(1, 0.2, low=0.5, high=1.5))\n",
    "            \n",
    "            # Hill saturation parameters\n",
    "            K_sat = numpyro.sample(\"K_sat\", dist.Gamma(1, 1))\n",
    "            S_sat = numpyro.sample(\"S_sat\", dist.Gamma(1, 1))\n",
    "            nu_sat = numpyro.sample(\"nu_sat\", dist.Gamma(1, 1))\n",
    "            \n",
    "            # Competitive effect parameters\n",
    "            if competitor_data is not None:\n",
    "                phi_comp = numpyro.sample(\"phi_comp\", dist.Gamma(1, 1))\n",
    "                kappa_comp = numpyro.sample(\"kappa_comp\", dist.Gamma(1, 1))\n",
    "            else:\n",
    "                phi_comp = kappa_comp = None\n",
    "        \n",
    "        # Cross-channel interaction parameters with shrinkage\n",
    "        if n_interactions > 0:\n",
    "            tau_global = numpyro.sample(\"tau_global\", dist.HalfCauchy(0.1))\n",
    "            delta_interact = numpyro.sample(\"delta_interact\", \n",
    "                                          dist.Normal(0, tau_global).expand([n_interactions]))\n",
    "        else:\n",
    "            delta_interact = jnp.array([])\n",
    "        \n",
    "        # Extra features coefficients\n",
    "        if n_extra_features > 0:\n",
    "            gamma_extra = numpyro.sample(\"gamma_extra\", \n",
    "                                       dist.Normal(0, 1).expand([n_extra_features]))\n",
    "        \n",
    "        # Error term\n",
    "        sigma = numpyro.sample(\"sigma\", dist.HalfNormal(1))\n",
    "        \n",
    "        # === TRANSFORMATIONS ===\n",
    "        \n",
    "        # Apply media transformations\n",
    "        transformed_media = []\n",
    "        for i in range(n_media_channels):\n",
    "            # Step 1: Power adstock\n",
    "            adstocked = self._apply_power_adstock(\n",
    "                media_data[:, i], lambda_adstock[i], rho_power[i]\n",
    "            )\n",
    "            \n",
    "            # Step 2: Hill saturation\n",
    "            saturated = self._apply_hill_saturation(\n",
    "                adstocked, K_sat[i], S_sat[i], nu_sat[i]\n",
    "            )\n",
    "            \n",
    "            # Step 3: Competitive effect\n",
    "            if competitor_data is not None:\n",
    "                final_media = self._apply_competitive_effect(\n",
    "                    saturated, competitor_data[:, i], phi_comp[i], kappa_comp[i]\n",
    "                )\n",
    "            else:\n",
    "                final_media = saturated\n",
    "            \n",
    "            transformed_media.append(final_media)\n",
    "        \n",
    "        transformed_media = jnp.stack(transformed_media, axis=1)\n",
    "        \n",
    "        # === MODEL PREDICTION ===\n",
    "        \n",
    "        # Time trend\n",
    "        time_arange = jnp.arange(n_time_periods, dtype=jnp.float32)\n",
    "        trend_contribution = trend_coef * time_arange\n",
    "        \n",
    "        # Seasonality contribution\n",
    "        seasonality_contribution = jnp.sum(seasonality_coef * seasonality_features, axis=1)\n",
    "        \n",
    "        # Time-varying media coefficients\n",
    "        seasonal_factor = jnp.cos(2 * jnp.pi * time_arange / 52.0)  # shape: (n_time,)\n",
    "\n",
    "        # Reshape for broadcasting\n",
    "        beta_media = beta_media[None, :]  # shape: (1, n_media)\n",
    "        seasonal_factor = seasonal_factor[:, None]  # shape: (n_time, 1)\n",
    "        seasonal_multiplier = seasonal_multiplier[None, :]  # shape: (1, n_media)\n",
    "\n",
    "        # Element-wise modulation: shape (n_time, n_media)\n",
    "        time_varying_beta = beta_media * (1 + seasonal_multiplier * seasonal_factor)\n",
    "\n",
    "        \n",
    "        # Media contribution with time-varying coefficients\n",
    "        media_contribution = jnp.sum(time_varying_beta * transformed_media, axis=1)\n",
    "        \n",
    "        # Cross-channel interactions\n",
    "        interaction_contribution = self._compute_cross_channel_interactions(\n",
    "            transformed_media, delta_interact\n",
    "        )\n",
    "        \n",
    "        # Extra features contribution\n",
    "        extra_contribution = (jnp.sum(gamma_extra * extra_features, axis=1) \n",
    "                            if n_extra_features > 0 else 0.0)\n",
    "        \n",
    "        # Total prediction\n",
    "        mu = (intercept + \n",
    "              trend_contribution + \n",
    "              seasonality_contribution + \n",
    "              media_contribution + \n",
    "              interaction_contribution + \n",
    "              extra_contribution)\n",
    "        \n",
    "        # Observation model\n",
    "        numpyro.sample(\"target\", dist.Normal(mu, sigma), obs=target_data)\n",
    "    \n",
    "    def fit(self, media_data: np.ndarray, \n",
    "            target_data: np.ndarray,\n",
    "            extra_features: Optional[np.ndarray] = None,\n",
    "            competitor_data: Optional[np.ndarray] = None,\n",
    "            num_warmup: int = 1000,\n",
    "            num_samples: int = 1000,\n",
    "            num_chains: int = 2,\n",
    "            target_accept_prob: float = 0.8,\n",
    "            max_tree_depth: int = 10,\n",
    "            use_svi: bool = False,\n",
    "            svi_num_steps: int = 10000) -> None:\n",
    "        \"\"\"\n",
    "        Fit the enhanced MMM model\n",
    "        \n",
    "        Args:\n",
    "            media_data: Media spend data [n_time_periods, n_media_channels]\n",
    "            target_data: Target variable (e.g., sales, conversions)\n",
    "            extra_features: Additional control variables\n",
    "            competitor_data: Competitor spend data\n",
    "            num_warmup: Number of warmup samples for MCMC\n",
    "            num_samples: Number of samples for MCMC\n",
    "            num_chains: Number of MCMC chains\n",
    "            target_accept_prob: Target acceptance probability\n",
    "            max_tree_depth: Maximum tree depth for NUTS\n",
    "            use_svi: Whether to use SVI instead of MCMC\n",
    "            svi_num_steps: Number of SVI optimization steps\n",
    "        \"\"\"\n",
    "        \n",
    "        # Convert to JAX arrays\n",
    "        media_data_jax = jnp.array(media_data)\n",
    "        target_data_jax = jnp.array(target_data)\n",
    "        extra_features_jax = jnp.array(extra_features) if extra_features is not None else None\n",
    "        competitor_data_jax = jnp.array(competitor_data) if competitor_data is not None else None\n",
    "        \n",
    "        # Model arguments\n",
    "        model_args = {\n",
    "            'media_data': media_data_jax,\n",
    "            'target_data': target_data_jax,\n",
    "            'extra_features': extra_features_jax,\n",
    "            'competitor_data': competitor_data_jax\n",
    "        }\n",
    "        \n",
    "        if use_svi:\n",
    "            # Variational inference\n",
    "            self.guide = AutoLowRankMultivariateNormal(self._model_definition, rank=20)\n",
    "            optimizer = numpyro.optim.Adam(step_size=0.01)\n",
    "            self.svi = SVI(self._model_definition, self.guide, optimizer, loss=Trace_ELBO())\n",
    "            \n",
    "            # Run SVI\n",
    "            svi_result = self.svi.run(\n",
    "                jax.random.PRNGKey(0), \n",
    "                svi_num_steps, \n",
    "                **model_args\n",
    "            )\n",
    "            \n",
    "            # Get posterior samples\n",
    "            posterior_samples = Predictive(\n",
    "                self.guide, \n",
    "                params=svi_result.params, \n",
    "                num_samples=num_samples\n",
    "            )(jax.random.PRNGKey(1), **{k: v for k, v in model_args.items() if k != 'target_data'})\n",
    "            \n",
    "            self.posterior_samples = posterior_samples\n",
    "            \n",
    "        else:\n",
    "            # MCMC inference\n",
    "            nuts_kernel = NUTS(\n",
    "                self._model_definition,\n",
    "                target_accept_prob=target_accept_prob,\n",
    "                max_tree_depth=max_tree_depth\n",
    "            )\n",
    "            \n",
    "            self.mcmc = MCMC(\n",
    "                nuts_kernel,\n",
    "                num_warmup=num_warmup,\n",
    "                num_samples=num_samples,\n",
    "                num_chains=num_chains\n",
    "            )\n",
    "            \n",
    "            # Run MCMC\n",
    "            self.mcmc.run(jax.random.PRNGKey(0), **model_args)\n",
    "            self.posterior_samples = self.mcmc.get_samples()\n",
    "    \n",
    "    def predict(self, media_data: np.ndarray, \n",
    "                extra_features: Optional[np.ndarray] = None,\n",
    "                competitor_data: Optional[np.ndarray] = None,\n",
    "                num_samples: int = 1000) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Generate predictions from the fitted model\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with predictions and uncertainty intervals\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.posterior_samples is None:\n",
    "            raise ValueError(\"Model must be fitted before making predictions\")\n",
    "        \n",
    "        # Convert to JAX arrays\n",
    "        media_data_jax = jnp.array(media_data)\n",
    "        extra_features_jax = jnp.array(extra_features) if extra_features is not None else None\n",
    "        competitor_data_jax = jnp.array(competitor_data) if competitor_data is not None else None\n",
    "        \n",
    "        # Predictive model\n",
    "        predictive = Predictive(\n",
    "            self._model_definition,\n",
    "            posterior_samples=self.posterior_samples,\n",
    "            num_samples=num_samples\n",
    "        )\n",
    "        \n",
    "        # Generate predictions\n",
    "        predictions = predictive(\n",
    "            jax.random.PRNGKey(1),\n",
    "            media_data=media_data_jax,\n",
    "            extra_features=extra_features_jax,\n",
    "            competitor_data=competitor_data_jax\n",
    "        )\n",
    "        \n",
    "        pred_target = predictions['target']\n",
    "        \n",
    "        return {\n",
    "            'predictions': np.array(pred_target),\n",
    "            'mean': np.mean(pred_target, axis=0),\n",
    "            'median': np.median(pred_target, axis=0),\n",
    "            'lower_ci': np.percentile(pred_target, 2.5, axis=0),\n",
    "            'upper_ci': np.percentile(pred_target, 97.5, axis=0)\n",
    "        }\n",
    "    \n",
    "    def compute_media_contributions(self, media_data: np.ndarray,\n",
    "                                  extra_features: Optional[np.ndarray] = None,\n",
    "                                  competitor_data: Optional[np.ndarray] = None) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Compute individual media channel contributions\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with contribution decomposition\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.posterior_samples is None:\n",
    "            raise ValueError(\"Model must be fitted before computing contributions\")\n",
    "        \n",
    "        # Convert to JAX arrays\n",
    "        media_data_jax = jnp.array(media_data)\n",
    "        extra_features_jax = jnp.array(extra_features) if extra_features is not None else None\n",
    "        competitor_data_jax = jnp.array(competitor_data) if competitor_data is not None else None\n",
    "        \n",
    "        n_time_periods, n_media_channels = media_data.shape\n",
    "        n_samples = len(list(self.posterior_samples.values())[0])\n",
    "        \n",
    "        # Extract parameters\n",
    "        beta_media = self.posterior_samples['beta_media']\n",
    "        seasonal_multiplier = self.posterior_samples['seasonal_multiplier']\n",
    "        lambda_adstock = self.posterior_samples['lambda_adstock']\n",
    "        rho_power = self.posterior_samples['rho_power']\n",
    "        K_sat = self.posterior_samples['K_sat']\n",
    "        S_sat = self.posterior_samples['S_sat']\n",
    "        nu_sat = self.posterior_samples['nu_sat']\n",
    "        \n",
    "        # Compute contributions for each sample\n",
    "        contributions = np.zeros((n_samples, n_time_periods, n_media_channels))\n",
    "        \n",
    "        for s in range(n_samples):\n",
    "            # Time-varying coefficients\n",
    "            time_arange = jnp.arange(n_time_periods, dtype=jnp.float32)\n",
    "            seasonal_factor = jnp.cos(2 * jnp.pi * time_arange / 52.0)\n",
    "            \n",
    "            for c in range(n_media_channels):\n",
    "                # Apply transformations\n",
    "                adstocked = self._apply_power_adstock(\n",
    "                    media_data_jax[:, c], lambda_adstock[s, c], rho_power[s, c]\n",
    "                )\n",
    "                \n",
    "                saturated = self._apply_hill_saturation(\n",
    "                    adstocked, K_sat[s, c], S_sat[s, c], nu_sat[s, c]\n",
    "                )\n",
    "                \n",
    "                if competitor_data is not None:\n",
    "                    phi_comp = self.posterior_samples['phi_comp'][s, c]\n",
    "                    kappa_comp = self.posterior_samples['kappa_comp'][s, c]\n",
    "                    final_media = self._apply_competitive_effect(\n",
    "                        saturated, competitor_data_jax[:, c], phi_comp, kappa_comp\n",
    "                    )\n",
    "                else:\n",
    "                    final_media = saturated\n",
    "                \n",
    "                # Time-varying coefficient\n",
    "                seasonal_factor = seasonal_factor.reshape(-1, 1, 1)\n",
    "\n",
    "                time_varying_beta = beta_media[None, :, :] * ( 1 + seasonal_multiplier[None, :, :] * seasonal_factor)\n",
    "\n",
    "                # Channel contribution\n",
    "                final_media_reshaped = final_media[:, None, None] \n",
    "                contributions[s, :, c] = np.sum(time_varying_beta[:, s, c] * media_data[:, c])\n",
    "\n",
    "        return {\n",
    "            'contributions': contributions,\n",
    "            'mean_contributions': np.mean(contributions, axis=0),\n",
    "            'total_contribution': np.sum(np.mean(contributions, axis=0), axis=1)\n",
    "        }\n",
    "    \n",
    "    def optimize_budget(self, total_budget: float,\n",
    "                       media_data_historical: np.ndarray,\n",
    "                       extra_features: Optional[np.ndarray] = None,\n",
    "                       competitor_data: Optional[np.ndarray] = None,\n",
    "                       bounds: Optional[Dict[str, Tuple[float, float]]] = None,\n",
    "                       risk_tolerance: float = 0.1,\n",
    "                       num_samples: int = 500) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Optimize budget allocation with risk adjustment\n",
    "        \n",
    "        Args:\n",
    "            total_budget: Total budget to allocate\n",
    "            media_data_historical: Historical media data for baseline\n",
    "            bounds: Optional bounds for each channel {channel_idx: (min, max)}\n",
    "            risk_tolerance: Risk tolerance parameter (higher = more risk averse)\n",
    "            num_samples: Number of posterior samples for uncertainty quantification\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with optimal allocation and expected returns\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.posterior_samples is None:\n",
    "            raise ValueError(\"Model must be fitted before optimizing budget\")\n",
    "        \n",
    "        n_media_channels = media_data_historical.shape[1]\n",
    "        \n",
    "        # Default bounds: 0 to total budget for each channel\n",
    "        if bounds is None:\n",
    "            bounds_list = [(0, total_budget) for _ in range(n_media_channels)]\n",
    "        else:\n",
    "            bounds_list = [bounds.get(i, (0, total_budget)) for i in range(n_media_channels)]\n",
    "        \n",
    "        def objective(allocation):\n",
    "            \"\"\"Risk-adjusted objective function\"\"\"\n",
    "            # Ensure budget constraint\n",
    "            if np.sum(allocation) > total_budget * 1.001:  # Small tolerance\n",
    "                return -1e6\n",
    "            \n",
    "            # Create hypothetical media data\n",
    "            hypothetical_media = np.zeros_like(media_data_historical)\n",
    "            hypothetical_media[-1] = allocation  # Allocate to last period\n",
    "            \n",
    "            # Predict outcomes for subset of samples\n",
    "            sample_indices = np.random.choice(\n",
    "                len(list(self.posterior_samples.values())[0]), \n",
    "                size=num_samples, \n",
    "                replace=False\n",
    "            )\n",
    "            \n",
    "            predicted_outcomes = []\n",
    "            \n",
    "            for idx in sample_indices:\n",
    "                # Extract single sample\n",
    "                sample = {k: v[idx] for k, v in self.posterior_samples.items()}\n",
    "                \n",
    "                # Predict with this sample\n",
    "                pred = self._predict_with_sample(\n",
    "                    sample, hypothetical_media, extra_features, competitor_data\n",
    "                )\n",
    "                predicted_outcomes.append(pred[-1])  # Last period prediction\n",
    "            \n",
    "            predicted_outcomes = np.array(predicted_outcomes)\n",
    "            \n",
    "            # Risk-adjusted return\n",
    "            mean_return = np.mean(predicted_outcomes)\n",
    "            std_return = np.std(predicted_outcomes)\n",
    "            \n",
    "            return mean_return - risk_tolerance * std_return\n",
    "        \n",
    "        # Budget constraint\n",
    "        budget_constraint = {'type': 'eq', 'fun': lambda x: total_budget - np.sum(x)}\n",
    "        \n",
    "        # Initial guess: equal allocation\n",
    "        initial_guess = np.full(n_media_channels, total_budget / n_media_channels)\n",
    "        \n",
    "        # Optimize\n",
    "        result = minimize(\n",
    "            lambda x: -objective(x),  # Minimize negative objective\n",
    "            x0=initial_guess,\n",
    "            bounds=bounds_list,\n",
    "            constraints=budget_constraint,\n",
    "            method='SLSQP'\n",
    "        )\n",
    "        \n",
    "        optimal_allocation = result.x\n",
    "        expected_return = -result.fun\n",
    "        \n",
    "        return {\n",
    "            'optimal_allocation': optimal_allocation,\n",
    "            'expected_return': expected_return,\n",
    "            'allocation_shares': optimal_allocation / total_budget,\n",
    "            'optimization_success': result.success,\n",
    "            'optimization_message': result.message\n",
    "        }\n",
    "    \n",
    "    def _predict_with_sample(self, sample: Dict[str, float], \n",
    "                           media_data: np.ndarray,\n",
    "                           extra_features: Optional[np.ndarray] = None,\n",
    "                           competitor_data: Optional[np.ndarray] = None) -> np.ndarray:\n",
    "        \"\"\"Predict with a single posterior sample\"\"\"\n",
    "        \n",
    "        media_data_jax = jnp.array(media_data)\n",
    "        extra_features_jax = jnp.array(extra_features) if extra_features is not None else None\n",
    "        competitor_data_jax = jnp.array(competitor_data) if competitor_data is not None else None\n",
    "        \n",
    "        n_time_periods, n_media_channels = media_data.shape\n",
    "        \n",
    "        # Seasonality features\n",
    "        seasonality_features = self._create_seasonality_features(n_time_periods)\n",
    "        \n",
    "        # Extract parameters\n",
    "        intercept = sample['intercept']\n",
    "        trend_coef = sample['trend_coef']\n",
    "        seasonality_coef = sample['seasonality_coef']\n",
    "        beta_media = sample['beta_media']\n",
    "        seasonal_multiplier = sample['seasonal_multiplier']\n",
    "        \n",
    "        # Transform media data\n",
    "        transformed_media = []\n",
    "        for i in range(n_media_channels):\n",
    "            # Apply transformations\n",
    "            adstocked = self._apply_power_adstock(\n",
    "                media_data_jax[:, i], \n",
    "                sample['lambda_adstock'][i], \n",
    "                sample['rho_power'][i]\n",
    "            )\n",
    "            \n",
    "            saturated = self._apply_hill_saturation(\n",
    "                adstocked, \n",
    "                sample['K_sat'][i], \n",
    "                sample['S_sat'][i], \n",
    "                sample['nu_sat'][i]\n",
    "            )\n",
    "            \n",
    "            if competitor_data is not None:\n",
    "                final_media = self._apply_competitive_effect(\n",
    "                    saturated, \n",
    "                    competitor_data_jax[:, i], \n",
    "                    sample['phi_comp'][i], \n",
    "                    sample['kappa_comp'][i]\n",
    "                )\n",
    "            else:\n",
    "                final_media = saturated\n",
    "            \n",
    "            transformed_media.append(final_media)\n",
    "        \n",
    "        transformed_media = jnp.stack(transformed_media, axis=1)\n",
    "        \n",
    "        # Compute prediction\n",
    "        time_arange = jnp.arange(n_time_periods, dtype=jnp.float32)\n",
    "        \n",
    "        # Time trend\n",
    "        trend_contribution = trend_coef * time_arange\n",
    "        \n",
    "        # Seasonality\n",
    "        seasonality_contribution = jnp.sum(seasonality_coef * seasonality_features, axis=1)\n",
    "        \n",
    "        # Time-varying media effects\n",
    "        seasonal_factor = jnp.cos(2 * jnp.pi * time_arange / 52.0)  # (104,)\n",
    "        seasonal_effect = 1 + seasonal_multiplier * seasonal_factor[:, None]  # (104, 4)\n",
    "        time_varying_beta = beta_media[None, :] * seasonal_effect  # (104, 4)\n",
    "\n",
    "        media_contribution = jnp.sum(time_varying_beta * transformed_media, axis=1)\n",
    "        \n",
    "        # Cross-channel interactions\n",
    "        n_interactions = n_media_channels * (n_media_channels - 1) // 2\n",
    "        if n_interactions > 0:\n",
    "            delta_interact = sample['delta_interact']\n",
    "            interaction_contribution = self._compute_cross_channel_interactions(\n",
    "                transformed_media, delta_interact\n",
    "            )\n",
    "        else:\n",
    "            interaction_contribution = 0.0\n",
    "        \n",
    "        # Extra features\n",
    "        extra_contribution = (jnp.sum(sample['gamma_extra'] * extra_features_jax, axis=1) \n",
    "                            if extra_features_jax is not None else 0.0)\n",
    "        \n",
    "        # Total prediction\n",
    "        mu = (intercept + \n",
    "              trend_contribution + \n",
    "              seasonality_contribution + \n",
    "              media_contribution + \n",
    "              interaction_contribution + \n",
    "              extra_contribution)\n",
    "        \n",
    "        return np.array(mu)\n",
    "    \n",
    "    def get_model_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get summary statistics of the fitted model\"\"\"\n",
    "        \n",
    "        if self.posterior_samples is None:\n",
    "            raise ValueError(\"Model must be fitted before getting summary\")\n",
    "        \n",
    "        summary = {}\n",
    "        \n",
    "        for param_name, param_values in self.posterior_samples.items():\n",
    "            if param_values.ndim == 1:\n",
    "                summary[param_name] = {\n",
    "                    'mean': float(np.mean(param_values)),\n",
    "                    'std': float(np.std(param_values)),\n",
    "                    'median': float(np.median(param_values)),\n",
    "                    'q025': float(np.percentile(param_values, 2.5)),\n",
    "                    'q975': float(np.percentile(param_values, 97.5))\n",
    "                }\n",
    "            else:\n",
    "                # Multi-dimensional parameters\n",
    "                summary[param_name] = {\n",
    "                    'mean': np.mean(param_values, axis=0).tolist(),\n",
    "                    'std': np.std(param_values, axis=0).tolist(),\n",
    "                    'median': np.median(param_values, axis=0).tolist(),\n",
    "                    'q025': np.percentile(param_values, 2.5, axis=0).tolist(),\n",
    "                    'q975': np.percentile(param_values, 97.5, axis=0).tolist()\n",
    "                }\n",
    "        \n",
    "        return summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b47092f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Enhanced LightweightMMM...\n",
      "Fitting model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 1000/1000 [00:17<00:00, 57.20it/s, 1023 steps of size 9.48e-03. acc. prob=0.90]\n",
      "sample: 100%|██████████| 1000/1000 [00:15<00:00, 64.57it/s, 511 steps of size 8.89e-03. acc. prob=0.89]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fitted successfully!\n",
      "Generating predictions...\n",
      "Prediction MAPE: 3.22%\n",
      "Computing media contributions...\n",
      "Mean media contributions (last period):\n",
      "[217.01539944 663.73699353 240.52373062 229.29111752]\n",
      "Total contribution across time periods:\n",
      "[1350.56724111 1350.56724111 1350.56724111 1350.56724111 1350.56724111\n",
      " 1350.56724111 1350.56724111 1350.56724111 1350.56724111 1350.56724111\n",
      " 1350.56724111 1350.56724111 1350.56724111 1350.56724111 1350.56724111\n",
      " 1350.56724111 1350.56724111 1350.56724111 1350.56724111 1350.56724111\n",
      " 1350.56724111 1350.56724111 1350.56724111 1350.56724111 1350.56724111\n",
      " 1350.56724111 1350.56724111 1350.56724111 1350.56724111 1350.56724111\n",
      " 1350.56724111 1350.56724111 1350.56724111 1350.56724111 1350.56724111\n",
      " 1350.56724111 1350.56724111 1350.56724111 1350.56724111 1350.56724111\n",
      " 1350.56724111 1350.56724111 1350.56724111 1350.56724111 1350.56724111\n",
      " 1350.56724111 1350.56724111 1350.56724111 1350.56724111 1350.56724111\n",
      " 1350.56724111 1350.56724111 1350.56724111 1350.56724111 1350.56724111\n",
      " 1350.56724111 1350.56724111 1350.56724111 1350.56724111 1350.56724111\n",
      " 1350.56724111 1350.56724111 1350.56724111 1350.56724111 1350.56724111\n",
      " 1350.56724111 1350.56724111 1350.56724111 1350.56724111 1350.56724111\n",
      " 1350.56724111 1350.56724111 1350.56724111 1350.56724111 1350.56724111\n",
      " 1350.56724111 1350.56724111 1350.56724111 1350.56724111 1350.56724111\n",
      " 1350.56724111 1350.56724111 1350.56724111 1350.56724111 1350.56724111\n",
      " 1350.56724111 1350.56724111 1350.56724111 1350.56724111 1350.56724111\n",
      " 1350.56724111 1350.56724111 1350.56724111 1350.56724111 1350.56724111\n",
      " 1350.56724111 1350.56724111 1350.56724111 1350.56724111 1350.56724111\n",
      " 1350.56724111 1350.56724111 1350.56724111 1350.56724111]\n",
      "Optimizing budget...\n",
      "Optimal budget allocation:\n",
      "[25. 25. 25. 25.]\n",
      "Allocation shares:\n",
      "[0.25 0.25 0.25 0.25]\n",
      "Expected return:\n",
      "29.849201\n",
      "Optimization success: True\n",
      "Message: Optimization terminated successfully\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # synthetic data\n",
    "    np.random.seed(42)\n",
    "    n_time_periods = 104  \n",
    "    n_media_channels = 4\n",
    "    n_extra_features = 2\n",
    "    \n",
    "    # Generate synthetic media data\n",
    "    media_data = np.random.exponential(2, size=(n_time_periods, n_media_channels))\n",
    "    \n",
    "    # Generate synthetic competitor data\n",
    "    competitor_data = np.random.exponential(1.5, size=(n_time_periods, n_media_channels))\n",
    "    \n",
    "    # Generate synthetic extra features (e.g., price, promotions)\n",
    "    extra_features = np.random.normal(0, 1, size=(n_time_periods, n_extra_features))\n",
    "    \n",
    "    # Generate synthetic target data with known relationships\n",
    "    true_betas = np.array([0.5, 0.3, 0.7, 0.4])\n",
    "    true_gammas = np.array([0.2, -0.1])\n",
    "    \n",
    "    # Simple linear relationship for synthetic data\n",
    "    target_data = (10 + \n",
    "                   0.01 * np.arange(n_time_periods) +  # trend\n",
    "                   np.sum(true_betas * media_data, axis=1) +  # media effect\n",
    "                   np.sum(true_gammas * extra_features, axis=1) +  # control variables\n",
    "                   np.random.normal(0, 0.5, n_time_periods))  # noise\n",
    "    \n",
    "\n",
    "\n",
    "    print(\"Initializing Enhanced LightweightMMM...\")\n",
    "    model = EnhancedLightweightMMM(\n",
    "        model_name=\"enhanced_hill_adstock\",\n",
    "        degrees_seasonality=2,\n",
    "        weekday_seasonality=True\n",
    "    )\n",
    "    \n",
    "    print(\"Fitting model...\")\n",
    "    model.fit(\n",
    "        media_data=media_data,\n",
    "        target_data=target_data,\n",
    "        extra_features=extra_features,\n",
    "        competitor_data=competitor_data,\n",
    "        num_warmup=500,\n",
    "        num_samples=500,\n",
    "        num_chains=2,\n",
    "        use_svi=False  \n",
    "    )\n",
    "    \n",
    "    print(\"Model fitted successfully!\")\n",
    "\n",
    "    print(\"Generating predictions...\")\n",
    "    predictions = model.predict(\n",
    "        media_data=media_data,\n",
    "        extra_features=extra_features,\n",
    "        competitor_data=competitor_data\n",
    "    )\n",
    "    \n",
    "    print(f\"Prediction MAPE: {np.mean(np.abs((predictions['mean'] - target_data) / target_data)) * 100:.2f}%\")\n",
    "    \n",
    "    print(\"Computing media contributions...\")\n",
    "    contributions_result = model.compute_media_contributions(\n",
    "        media_data=media_data,\n",
    "        extra_features=extra_features,\n",
    "        competitor_data=competitor_data\n",
    "    )\n",
    "\n",
    "    mean_contributions = contributions_result['mean_contributions']\n",
    "    total_contribution = contributions_result['total_contribution']\n",
    "\n",
    "    print(\"Mean media contributions (last period):\")\n",
    "    print(mean_contributions[-1])\n",
    "\n",
    "    print(\"Total contribution across time periods:\")\n",
    "    print(total_contribution)\n",
    "\n",
    "    print(\"Optimizing budget...\")\n",
    "    total_budget = 100.0  \n",
    "    budget_result = model.optimize_budget(\n",
    "        total_budget=total_budget,\n",
    "        media_data_historical=media_data,\n",
    "        extra_features=extra_features,\n",
    "        competitor_data=competitor_data,\n",
    "        risk_tolerance=0.2\n",
    "    )\n",
    "\n",
    "    print(\"Optimal budget allocation:\")\n",
    "    print(budget_result['optimal_allocation'])\n",
    "\n",
    "    print(\"Allocation shares:\")\n",
    "    print(budget_result['allocation_shares'])\n",
    "\n",
    "    print(\"Expected return:\")\n",
    "    print(budget_result['expected_return'])\n",
    "\n",
    "    print(\"Optimization success:\", budget_result['optimization_success'])\n",
    "    print(\"Message:\", budget_result['optimization_message'])\n",
    "\n",
    "    print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
